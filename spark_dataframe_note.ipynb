{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame Command Note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "* [Intro](#intro)\n",
    "* [Installation](#install)\n",
    "* [Spark DataFrame API](#df)\n",
    "* [Spark SQL](#sql)\n",
    "* [Machine Learning in PySpark](#ml)\n",
    "    * [Linear Regression](#lm)\n",
    "    * [Kmeans](#kmeans)\n",
    "    * [RecSys: Alternative Least Square](#als)\n",
    "* [Reference](#refer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='intro'>Intro</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has two different kinds of APIs\n",
    "\n",
    "* **APIs**\n",
    "    * RDD API: lower level, we should use this when we deal with unstructured data\n",
    "    * DataFrame API: can be related to pandas dataframe in python.\n",
    "        * SparkSQL    \n",
    "* **2 modes**\n",
    "    * shell\n",
    "    * script\n",
    "    \n",
    "each block in the hadoop concept correspond to a partition of RDD.\n",
    "One file correspond to a RDD.\n",
    "\n",
    "* **Pros of Spark to MapReduce**: The main advantage of using Spark is that it can hold a portion of the original data in memory. It's easier to wrote any kinds of algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='install'>Installation on Mac</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Steps**:\n",
    "    1. download version spark-2.3.0-bin-hadoop2.7.tgz from [Spark downloads page](http://spark.apache.org/downloads.html)\n",
    "    2. manually unzip the file\n",
    "    3. sudo mv spark-2.3.0-bin-hadoop2.7 /opt/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "        * used sudo because we need permission to move files into opt folder\n",
    "    4. ln -s /opt/spark-2.3.0-bin-hadoop2.7 /opt/sparkÌ€\n",
    "        * create a shortcut to the actual folder\n",
    "    5. vi ~/.zshrc\n",
    "        * in other Linux the file should be ~/.bashrc\n",
    "        * I downloaded zsh for my command line, that's why I use this file\n",
    "    6. adding the following lines in zshrc file\n",
    "        * export SPARK_HOME=/opt/spark\n",
    "        * export PATH=$SPARK_HOME/bin:$PATH\n",
    "    7. create a new terminal tab, type `pyspark`\n",
    "    8. Used the second method to link pyspark to my jupyter notebook\n",
    "        * There is another and more generalized way to use PySpark in a Jupyter Notebook: use findSpark package to make a Spark Context available in your code.\n",
    "    9. run the sample code in below and it works.    \n",
    "      \n",
    "* **Reference**: [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "findspark.init(\"/opt/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14189504\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/47665491/pyspark-throws-typeerror-textfile-missing-1-required-positional-argument-na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not print anything unless it's an error\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='df'>Spark DataFrame API</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read csv file into Spark DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/Crimes_-_2001_to_present.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[databricks/spark-csv](https://github.com/databricks/spark-csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actual #rows is \n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create DataFrame from Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "scratch_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scratch_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Read Json into DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "customer = sqlContext.read.json(\"data/customer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(address=Row(city='New Orleans', state='LA', street='6649 N Blue Gum St', zip='70116'), first_name='James', last_name='Butterburg'),\n",
       " Row(address=Row(city='Brighton', state='MI', street='4 B Blue Ridge Blvd', zip='48116'), first_name='Josephine', last_name='Darakjy'),\n",
       " Row(address=Row(city='Bridgeport', state='NJ', street='8 W Cerritos Ave #54', zip='08014'), first_name='Art', last_name='Chemel')]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **show records, head**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|              Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|    010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...|067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|    026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "+--------+-----------+--------------------+-------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)'),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **take the first 1000 rows of a Spark Dataframe and return a new dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Stackoverflow](https://stackoverflow.com/questions/34206508/is-there-a-way-to-take-the-first-1000-rows-of-a-spark-dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df.limit(10).alias(\"test\").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get Column Types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: boolean, Domestic: boolean, Beat: int, District: int, Ward: int, Community Area: string, FBI Code: string, X Coordinate: int, Y Coordinate: int, Year: int, Updated On: string, Latitude: double, Longitude: double, Location: string]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'int'),\n",
       " ('Case Number', 'string'),\n",
       " ('Date', 'string'),\n",
       " ('Block', 'string'),\n",
       " ('IUCR', 'string'),\n",
       " ('Primary Type', 'string'),\n",
       " ('Description', 'string'),\n",
       " ('Location Description', 'string'),\n",
       " ('Arrest', 'boolean'),\n",
       " ('Domestic', 'boolean'),\n",
       " ('Beat', 'int'),\n",
       " ('District', 'int'),\n",
       " ('Ward', 'int'),\n",
       " ('Community Area', 'string'),\n",
       " ('FBI Code', 'string'),\n",
       " ('X Coordinate', 'int'),\n",
       " ('Y Coordinate', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Updated On', 'string'),\n",
       " ('Latitude', 'double'),\n",
       " ('Longitude', 'double'),\n",
       " ('Location', 'string')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the shape of dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5801844"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Describe the dataset; Get summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|summary|                ID|       Case Number|                Date|               Block|             IUCR|     Primary Type|    Description|Location Description|              Beat|          District|              Ward|    Community Area|          FBI Code|      X Coordinate|      Y Coordinate|              Year|          Updated On|           Latitude|          Longitude|            Location|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "|  count|           5801844|           5801841|             5801844|             5801842|          5801844|          5801844|        5801844|             5801009|           5801844|           4601628|           5186940|           5186069|           5801844|           5753924|           5753924|           5801844|             5801678|            5753924|            5753924|             5753924|\n",
      "|   mean| 5478809.816915622|       302402.4375|                null|                null|1134.731230109342|             null|           null|                null|1199.6529617135518|11.304880142419162|22.585335284387327|37.721575177442126|12.340449301040367| 1164505.101389417|1885629.2320944802|2006.9770088613207|                null|  41.84176525667321| -87.67185536981503|                null|\n",
      "| stddev|2587006.9227430555|136858.87616128215|                null|                null|807.5054229686023|             null|           null|                null| 704.5764004952774|6.9362124463080885|13.794330067588891|21.550102471312908|  7.41121340178233|16200.899281313013| 31439.98034154956|4.0104357430230335|                null|0.08646691031758802|0.05895851757394647|                null|\n",
      "|    min|               634|         01G050460|01/01/2001 01:00:...|  0000X  I94/EXIT 12|             0110|            ARSON| $300 AND UNDER|\"CTA \"\"L\"\" PLATFORM\"|               111|                 1|                 1|                  |               01A|           1092697|           1813930|              2001|01/01/2000 09:32:...|       41.644580105|      -87.934324986|(41.644580105, -8...|\n",
      "|    max|          10086206|         ZZZ199957|12/31/2014 12:59:...|175XX W WINSTON C...|             9901|WEAPONS VIOLATION|WIREROOM/SPORTS|                YMCA|              2535|                31|                50|                 9|                26|           1205152|           1951664|              2015|12/31/2014 12:39:...|       42.023024908|      -87.524388789|(42.023024908, -8...|\n",
      "+-------+------------------+------------------+--------------------+--------------------+-----------------+-----------------+---------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Select Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      ID|                Date|\n",
      "+--------+--------------------+\n",
      "|10078659|05/19/2015 11:57:...|\n",
      "|10078598|05/19/2015 11:50:...|\n",
      "|10078625|05/19/2015 11:47:...|\n",
      "|10078662|05/19/2015 11:46:...|\n",
      "|10078584|05/19/2015 11:45:...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('ID','Date').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Rename column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('oldName', 'newName')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get unique value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID',\n",
       " 'Case Number',\n",
       " 'Date',\n",
       " 'Block',\n",
       " 'IUCR',\n",
       " 'Primary Type',\n",
       " 'Description',\n",
       " 'Location Description',\n",
       " 'Arrest',\n",
       " 'Domestic',\n",
       " 'Beat',\n",
       " 'District',\n",
       " 'Ward',\n",
       " 'Community Area',\n",
       " 'FBI Code',\n",
       " 'X Coordinate',\n",
       " 'Y Coordinate',\n",
       " 'Year',\n",
       " 'Updated On',\n",
       " 'Latitude',\n",
       " 'Longitude',\n",
       " 'Location']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|District|\n",
      "+--------+\n",
      "|      31|\n",
      "|      12|\n",
      "|      22|\n",
      "|    null|\n",
      "|       1|\n",
      "|      13|\n",
      "|       6|\n",
      "|      16|\n",
      "|       3|\n",
      "|      20|\n",
      "|       5|\n",
      "|      19|\n",
      "|      15|\n",
      "|       9|\n",
      "|      17|\n",
      "|       4|\n",
      "|       8|\n",
      "|      23|\n",
      "|       7|\n",
      "|      10|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('District').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('District').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Explode array data into rows in spark; Dividing complex rows of dataframe to simple rows in Pyspark; Explode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default into of explode method should be a list [1,2,3]. If the value in the row is not in list format, we need to specify how do we want to split the value. For exmaple, if the format is a string \"1,2,3\". We need to write it as `explode(split(df.col3, \",\"))`\n",
    "\n",
    "* **First way**: use **withColumn**+**explode**\n",
    "    * Note: if we want to add a new column, use this.\n",
    "\n",
    "[StackOverFlow: Explode array data into rows in spark](https://stackoverflow.com/questions/44436856/explode-array-data-into-rows-in-spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second way**: use **select**+**explode**+**alias**\n",
    "    * select\n",
    "    * explode: to separate \n",
    "    * alias: to specify the name of new column\n",
    "    * Note: if we want to select and add a new column at the same time.\n",
    "\n",
    "[StackOverFlow: Explode in PySpark](https://stackoverflow.com/questions/38210507/explode-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "explode_df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "|   1|   A|   1|\n",
      "|   1|   A|   2|\n",
      "|   1|   A|   3|\n",
      "|   2|   B|   3|\n",
      "|   2|   B|   5|\n",
      "+----+----+----+\n",
      "\n",
      "+----+----+---------+-------+\n",
      "|col1|col2|     col3|newcol3|\n",
      "+----+----+---------+-------+\n",
      "|   1|   A|[1, 2, 3]|      1|\n",
      "|   1|   A|[1, 2, 3]|      2|\n",
      "|   1|   A|[1, 2, 3]|      3|\n",
      "|   2|   B|   [3, 5]|      3|\n",
      "|   2|   B|   [3, 5]|      5|\n",
      "+----+----+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cover out the original col3\n",
    "explode_df.withColumn(\"col3\", explode(explode_df.col3)).show()\n",
    "# create a new column\n",
    "explode_df.withColumn(\"newcol3\", explode(explode_df.col3)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|first_name|       city|\n",
      "+----------+-----------+\n",
      "|     James|New Orleans|\n",
      "| Josephine|   Brighton|\n",
      "|       Art| Bridgeport|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer.select(\"first_name\",\n",
    "                explode(\"address\").alias(\"contactInfo\")\n",
    "                \"address.city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df.select(\"col1\",\n",
    "                  \"col2\",\n",
    "                  explode(explode_df.col3).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples of non-list format column value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|col1|col2| col3|\n",
      "+----+----+-----+\n",
      "|   1|   A|1,2,3|\n",
      "|   2|   B|  3,5|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2 = sqlContext.createDataFrame([(1, \"A\", \"1,2,3\"), (2, \"B\", \"3,5\")],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+-------+\n",
      "|col1|col2| col3|newcol3|\n",
      "+----+----+-----+-------+\n",
      "|   1|   A|1,2,3|      1|\n",
      "|   1|   A|1,2,3|      2|\n",
      "|   1|   A|1,2,3|      3|\n",
      "|   2|   B|  3,5|      3|\n",
      "|   2|   B|  3,5|      5|\n",
      "+----+----+-----+-------+\n",
      "\n",
      "+----+----+-------+\n",
      "|col1|col2|newcol3|\n",
      "+----+----+-------+\n",
      "|   1|   A|      1|\n",
      "|   1|   A|      2|\n",
      "|   1|   A|      3|\n",
      "|   2|   B|      3|\n",
      "|   2|   B|      5|\n",
      "+----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df2.withColumn(\"newcol3\", explode(split(explode_df2.col3, \",\"))).show()\n",
    "\n",
    "explode_df2.select(\"col1\",\n",
    "                   \"col2\",\n",
    "                   explode(split(explode_df2.col3, \",\")).alias(\"newcol3\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get unique list for each column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+\n",
      "|col1|col2|           col3|\n",
      "+----+----+---------------+\n",
      "|   1|   A|[1, 1, 1, 2, 3]|\n",
      "|   2|   B|      [3, 5, 5]|\n",
      "|   3|   C|             []|\n",
      "+----+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "explode_df = sqlContext.createDataFrame([(1, \"A\", [1,1,1,2,3]), (2, \"B\", [3,5,5]), (3, \"C\", [])],[\"col1\", \"col2\", \"col3\"])\n",
    "explode_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_list(x):\n",
    "    return list(set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+---------+\n",
      "|col1|col2|           col3|     col4|\n",
      "+----+----+---------------+---------+\n",
      "|   1|   A|[1, 1, 1, 2, 3]|[1, 2, 3]|\n",
      "|   2|   B|      [3, 5, 5]|   [3, 5]|\n",
      "|   3|   C|             []|       []|\n",
      "+----+----+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udf1 = udf(get_unique_list, ArrayType(StringType()))\n",
    "explode_df.withColumn('col4',udf1('col3')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Convert pyspark string to date format**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow: Convert pyspark string to date format](https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**for spark version > 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|      ID|                 dt|\n",
      "+--------+-------------------+\n",
      "|10078659|2015-05-19 11:57:00|\n",
      "|10078598|2015-05-19 11:50:00|\n",
      "|10078625|2015-05-19 11:47:00|\n",
      "|10078662|2015-05-19 11:46:00|\n",
      "|10078584|2015-05-19 11:45:00|\n",
      "|10078629|2015-05-19 11:40:00|\n",
      "|10079225|2015-05-19 11:30:00|\n",
      "|10078594|2015-05-19 11:30:00|\n",
      "|10080768|2015-05-19 11:30:00|\n",
      "|10078618|2015-05-19 11:30:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"ID\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss').alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.withColumn(\"datetime\",\n",
    "           to_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|Year|          Updated On|    Latitude|    Longitude|            Location|           datetime|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Spark version < 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|      ID|                 dt|\n",
      "+--------+-------------------+\n",
      "|10078659|2015-05-19 11:57:00|\n",
      "|10078598|2015-05-19 11:50:00|\n",
      "|10078625|2015-05-19 11:47:00|\n",
      "|10078662|2015-05-19 11:46:00|\n",
      "|10078584|2015-05-19 11:45:00|\n",
      "|10078629|2015-05-19 11:40:00|\n",
      "|10079225|2015-05-19 11:30:00|\n",
      "|10078594|2015-05-19 11:30:00|\n",
      "|10080768|2015-05-19 11:30:00|\n",
      "|10078618|2015-05-19 11:30:00|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"ID\",\n",
    "           from_unixtime(unix_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss')).alias('dt')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)', datetime='2015-05-19 11:57:00'),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)', datetime='2015-05-19 11:50:00'),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, Year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)', datetime='2015-05-19 11:47:00')]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.withColumn(\"datetime\",\n",
    "           from_unixtime(unix_timestamp(df2.Date, 'MM/dd/yyyy HH:mm:ss'))).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Spark DataFrame TimestampType - how to get Year, Month, Day values from field?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [StackOverFlow: year/month/dayofmonth](https://stackoverflow.com/questions/30949202/spark-dataframe-timestamptype-how-to-get-year-month-day-values-from-field)\n",
    "* [StackOverFlow: weekday](https://stackoverflow.com/questions/38928919/how-to-get-the-weekday-from-day-of-month-using-pyspark/38931967)\n",
    "* [pyspark sql function](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get year/month/day of month\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, minute, to_date\n",
    "df4 = df3.withColumn(\"year\", year(df3.datetime))\n",
    "df4 = df4.withColumn(\"month\", month(df4.datetime))\n",
    "df4 = df4.withColumn(\"dayofmonth\", dayofmonth(df4.datetime))\n",
    "df4 = df4.withColumn(\"hour\", hour(df4.datetime))\n",
    "df4 = df4.withColumn(\"minute\", minute(df4.datetime))                                        \n",
    "df4 = df4.withColumn(\"todate\", to_date(df4.datetime))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=10078659, Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic=False, Beat=624, District=6, Ward=8, Community Area='44', FBI Code='15', X Coordinate=1184626, Y Coordinate=1852799, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.751242944, Longitude=-87.599004724, Location='(41.751242944, -87.599004724)', datetime=datetime.datetime(2015, 5, 19, 11, 57), month=5, dayofmonth=19, hour=11, minute=57, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078598, Case Number='HY267408', Date='05/19/2015 11:50:00 PM', Block='067XX N SHERIDAN RD', IUCR='3731', Primary Type='INTERFERENCE WITH PUBLIC OFFICER', Description='OBSTRUCTING IDENTIFICATION', Location Description='STREET', Arrest=True, Domestic=False, Beat=2432, District=24, Ward=49, Community Area='1', FBI Code='24', X Coordinate=1167071, Y Coordinate=1944859, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=42.004255918, Longitude=-87.660691083, Location='(42.004255918, -87.660691083)', datetime=datetime.datetime(2015, 5, 19, 11, 50), month=5, dayofmonth=19, hour=11, minute=50, todate=datetime.date(2015, 5, 19)),\n",
       " Row(ID=10078625, Case Number='HY267417', Date='05/19/2015 11:47:00 PM', Block='026XX E 77TH ST', IUCR='2170', Primary Type='NARCOTICS', Description='POSSESSION OF DRUG EQUIPMENT', Location Description='STREET', Arrest=True, Domestic=False, Beat=421, District=4, Ward=7, Community Area='43', FBI Code='18', X Coordinate=1195299, Y Coordinate=1854463, year=2015, Updated On='05/26/2015 12:42:06 PM', Latitude=41.755552462, Longitude=-87.559839339, Location='(41.755552462, -87.559839339)', datetime=datetime.datetime(2015, 5, 19, 11, 47), month=5, dayofmonth=19, hour=11, minute=47, todate=datetime.date(2015, 5, 19))]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+\n",
      "|           datetime|dow_number|dow_string|\n",
      "+-------------------+----------+----------+\n",
      "|2015-05-19 11:57:00|         2|       Tue|\n",
      "|2015-05-19 11:50:00|         2|       Tue|\n",
      "|2015-05-19 11:47:00|         2|       Tue|\n",
      "|2015-05-19 11:46:00|         2|       Tue|\n",
      "|2015-05-19 11:45:00|         2|       Tue|\n",
      "|2015-05-19 11:40:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "|2015-05-19 11:30:00|         2|       Tue|\n",
      "+-------------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get weekday string /number\n",
    "from pyspark.sql.functions import date_format\n",
    "df4.select('datetime', date_format('datetime', 'u').alias('dow_number'), date_format('datetime', 'E').alias('dow_string')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|      ID|Case Number|                Date|               Block|IUCR|        Primary Type|         Description|Location Description|Arrest|Domestic|Beat|District|Ward|Community Area|FBI Code|X Coordinate|Y Coordinate|year|          Updated On|    Latitude|    Longitude|            Location|           datetime|month|dayofmonth|dow_string|dow_number|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "|10078659|   HY267429|05/19/2015 11:57:...|     010XX E 79TH ST|143A|   WEAPONS VIOLATION|UNLAWFUL POSS OF ...|              STREET|  true|   false| 624|       6|   8|            44|      15|     1184626|     1852799|2015|05/26/2015 12:42:...|41.751242944|-87.599004724|(41.751242944, -8...|2015-05-19 11:57:00|    5|        19|       Tue|         2|\n",
      "|10078598|   HY267408|05/19/2015 11:50:...| 067XX N SHERIDAN RD|3731|INTERFERENCE WITH...|OBSTRUCTING IDENT...|              STREET|  true|   false|2432|      24|  49|             1|      24|     1167071|     1944859|2015|05/26/2015 12:42:...|42.004255918|-87.660691083|(42.004255918, -8...|2015-05-19 11:50:00|    5|        19|       Tue|         2|\n",
      "|10078625|   HY267417|05/19/2015 11:47:...|     026XX E 77TH ST|2170|           NARCOTICS|POSSESSION OF DRU...|              STREET|  true|   false| 421|       4|   7|            43|      18|     1195299|     1854463|2015|05/26/2015 12:42:...|41.755552462|-87.559839339|(41.755552462, -8...|2015-05-19 11:47:00|    5|        19|       Tue|         2|\n",
      "|10078662|   HY267423|05/19/2015 11:46:...|     015XX E 62ND ST|051A|             ASSAULT| AGGRAVATED: HANDGUN|           APARTMENT| false|    true| 314|       3|   5|            42|     04A|     1187377|     1864316|2015|05/26/2015 12:42:...|41.782781732|-87.588558362|(41.782781732, -8...|2015-05-19 11:46:00|    5|        19|       Tue|         2|\n",
      "|10078584|   HY267397|05/19/2015 11:45:...|054XX S PRINCETON...|4625|       OTHER OFFENSE|    PAROLE VIOLATION|         GAS STATION|  true|   false| 935|       9|   3|            37|      26|     1175180|     1868551|2015|05/26/2015 12:42:...|41.794684214|-87.633149481|(41.794684214, -8...|2015-05-19 11:45:00|    5|        19|       Tue|         2|\n",
      "|10078629|   HY267393|05/19/2015 11:40:...|013XX S LAWNDALE AVE|0454|             BATTERY|AGG PO HANDS NO/M...|              STREET|  true|   false|1011|      10|  24|            29|     08B|     1151957|     1893696|2015|05/26/2015 12:42:...|41.864172884|-87.717647622|(41.864172884, -8...|2015-05-19 11:40:00|    5|        19|       Tue|         2|\n",
      "|10079225|   HY267395|05/19/2015 11:30:...|   064XX S LAFLIN ST|0497|             BATTERY|AGGRAVATED DOMEST...|           APARTMENT| false|    true| 725|       7|  17|            67|     04B|     1167397|     1862229|2015|05/26/2015 12:42:...|41.777506284|-87.661870632|(41.777506284, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078594|   HY267388|05/19/2015 11:30:...|   021XX W NORTH AVE|1305|     CRIMINAL DAMAGE| CRIMINAL DEFACEMENT|            SIDEWALK|  true|   false|1434|      14|  32|            24|      14|     1162022|     1910669|2015|05/26/2015 12:42:...|  41.9105442|-87.680225036|(41.9105442, -87....|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10080768|   HY269123|05/19/2015 11:30:...| 008XX N KILDARE AVE|0820|               THEFT|      $500 AND UNDER|           RESIDENCE| false|   false|1111|      11|  37|            23|      06|     1147592|     1905491|2015|05/26/2015 12:42:...|41.896624505|-87.733368852|(41.896624505, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "|10078618|   HY267392|05/19/2015 11:30:...|062XX S KOMENSKY AVE|0320|             ROBBERY|STRONGARM - NO WE...|            SIDEWALK| false|   false| 813|       8|  13|            65|      03|     1150396|     1863112|2015|05/26/2015 12:42:...|41.780276746|-87.724174049|(41.780276746, -8...|2015-05-19 11:30:00|    5|        19|       Tue|         2|\n",
      "+--------+-----------+--------------------+--------------------+----+--------------------+--------------------+--------------------+------+--------+----+--------+----+--------------+--------+------------+------------+----+--------------------+------------+-------------+--------------------+-------------------+-----+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"dow_string\", date_format(df4.datetime, 'E')).withColumn(\"dow_number\", date_format(df4.datetime, 'u')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create new column and name it; groupby agg pyspark rename**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# countDistnct can be any function from pyspark.sql\n",
    "df_indegree = df_sample.groupby('user2').agg(countDistinct('user1').alias(\"in_degree\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **create new column using max pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/37838361/withcolumn-not-allowing-me-to-use-max-function-to-generate-a-new-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import greatest\n",
    "\n",
    "a.withColumn(\"max_col\", greatest(a[\"one\"], a[\"two\"], a[\"three\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Convert 12 hours to 24 hours; Convert 12 hour into 24 hour times**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://codereview.stackexchange.com/questions/111596/12-hours-to-24-hours-conversion-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_24(time):\n",
    "    \"\"\"str -> str\n",
    "    converts 12 hours time format to 24 hours\n",
    "    \"\"\"\n",
    "    if time[-2:]==\"PM\":\n",
    "        if int(time[11:13]) == 12:\n",
    "            final_time = time[:-3]\n",
    "        else:    \n",
    "            final_time = time[:11] + str(int(time[11:13]) + 12) + time[13:19]\n",
    "    elif time[-2:]==\"AM\":\n",
    "        if int(time[11:13]) == 12:\n",
    "            final_time = time[:11] + str(\"00\") + time[13:19]\n",
    "        else:\n",
    "            final_time = time[:-3]\n",
    "    else:    \n",
    "        final_time = time\n",
    "    return final_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf1 = udf(convert_to_24,StringType())\n",
    "df = df.withColumn('myDate',udf1('Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Define output type for user defined function(UDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [UDF](http://changhsinlee.com/pyspark-udf/)\n",
    "* [UDF Return types](https://spark.apache.org/docs/2.1.2/api/python/_modules/pyspark/sql/types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__all__ = [\n",
    "    \"DataType\", \"NullType\", \"StringType\", \"BinaryType\", \"BooleanType\", \"DateType\",\n",
    "    \"TimestampType\", \"DecimalType\", \"DoubleType\", \"FloatType\", \"ByteType\", \"IntegerType\",\n",
    "    \"LongType\", \"ShortType\", \"ArrayType\", \"MapType\", \"StructField\", \"StructType\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# udf output as list\n",
    "\n",
    "# emoji related link\n",
    "# https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "# http://worthavisit.blogspot.com/2014/07/string-punctuation-in-python.html\n",
    "# https://stackoverflow.com/questions/43727583/expected-string-or-bytes-like-object\n",
    "def emoji_extract(des):\n",
    "    return re.findall(r'[^\\w\\s!,]', str(des))\n",
    "\n",
    "udf1 = udf(emoji_extract, ArrayType(StringType()))\n",
    "df2 = df_sample.withColumn('emoji',udf1('description'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **calculate pair wise frequency of categorical columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|Beat_District| 10| 11| 14| 24|  3|  4|  6|  7|  8|  9|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "|         2432|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|\n",
      "|          421|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|\n",
      "|         1111|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          725|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|\n",
      "|         1434|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|         1011|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|          314|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|\n",
      "|          813|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|\n",
      "|          624|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|          935|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|\n",
      "+-------------+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df4.crosstab('Beat', 'District')\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Drop rows with NA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Change Column Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[change a Dataframe column from String type to Double type in pyspark](https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: string, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Beat_District: double, 10: bigint, 11: bigint, 14: bigint, 24: bigint, 3: bigint, 4: bigint, 6: bigint, 7: bigint, 8: bigint, 9: bigint]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.withColumn(\"Beat_District\", df5[\"Beat_District\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **GroupBy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntax is similar to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.groupby('Age').agg({'Purchase': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.groupby('Age').max(\"height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Count Distinct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/36064777/get-the-distinct-elements-of-each-group-by-other-field-on-a-spark-1-6-dataframe/36137905)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_outdegree = df.groupby('user1').agg(countDistinct('user2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Sort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort using single column\n",
    "df.sort(df.age.desc()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **order by: sorting using multiple column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.orderBy([\"age\",\"city\"],ascending=[0,1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get the NULL/NaN percentage**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/33900726/count-number-of-non-nan-entries-in-each-column-of-spark-dataframe-with-pyspark?noredirect=1&lq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get number of not null values in each column\n",
    "df.agg(*[count_not_null(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get percentage of not null values in each column\n",
    "exprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in df.columns]\n",
    "df.agg(*exprs).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter NULL rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.where(col(\"month\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# count number of rows with NULL\n",
    "df.where(col(\"month\").isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get only not null rows\n",
    "df.where(col(\"month\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Fill na values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/42312042/how-to-replace-all-null-values-of-a-dataframe-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.filter(df['age']>24).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use == for equal\n",
    "df.filter(df['age']==24).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **filter by length of a column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/33695389/filtering-dataframe-using-the-length-of-a-column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   tokens|\n",
      "+---------+\n",
      "|[S, U, S]|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    ([\"L\", \"S\", \"Y\", \"S\"],  ),\n",
    "    ([\"L\", \"V\", \"I\", \"S\"],  ),\n",
    "    ([\"I\", \"A\", \"N\", \"A\"],  ),\n",
    "    ([\"I\", \"L\", \"S\", \"A\"],  ),\n",
    "    ([\"E\", \"N\", \"N\", \"Y\"],  ),\n",
    "    ([\"E\", \"I\", \"M\", \"A\"],  ),\n",
    "    ([\"O\", \"A\", \"N\", \"A\"],  ),\n",
    "    ([\"S\", \"U\", \"S\"],  )], \n",
    "    (\"tokens\", ))\n",
    "\n",
    "df.where(size(col(\"tokens\")) <= 3).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Replace values in a certain column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace AM, PM with space in Date column\n",
    "df.withColumn('Date', regexp_replace('Date', ' PM', '')).withColumn('Date', regexp_replace('Date', ' AM', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create lag/shift column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/34295642/spark-add-new-column-to-dataframe-with-value-from-previous-row)\n",
    "\n",
    "[StackOverFlow](https://stackoverflow.com/questions/37754704/how-to-implement-lead-and-lag-in-spark-scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# when using window function, for some reason we need to use HiveContext instead of SQLContext on wolf\n",
    "from pyspark.sql import HiveContext\n",
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| id|num|new_col|\n",
      "+---+---+-------+\n",
      "|  1|5.0|   null|\n",
      "|  2|3.0|    5.0|\n",
      "|  3|7.0|    3.0|\n",
      "|  4|9.0|    7.0|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag, col, lead\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df = sc.parallelize([(4, 9.0), (3, 7.0), (2, 3.0), (1, 5.0)]).toDF([\"id\", \"num\"])\n",
    "w = Window().partitionBy().orderBy(col(\"id\"))\n",
    "df.select(\"*\", lag(\"num\").over(w).alias(\"new_col\")).show() #.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| id|num|new_col|\n",
      "+---+---+-------+\n",
      "|  1|5.0|    3.0|\n",
      "|  2|3.0|    7.0|\n",
      "|  3|7.0|    9.0|\n",
      "|  4|9.0|   null|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", lead(\"num\").over(w).alias(\"new_col\")).show() #.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **dataframe join**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**how** â€“ str, default â€˜innerâ€™. One of **inner**, **outer**, **left_outer**, **right_outer**, **leftsemi**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.join(df2, 'name', 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Reference](https://forums.databricks.com/questions/876/is-there-a-better-method-to-join-two-dataframes-an.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join two dataframes and not have a duplicated column?\n",
    "df1.join(df2, df1.district == df2.DISTRICT, 'left_outer').drop(df2.DISTRICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will make sure that we don't have duplicated output key at least for pyspark version 1.4\n",
    "df.join(df4, ['name', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Dataframe to RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **keep the row with max value for a column and keep all columns (for max records per group)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[How to max value and keep all columns (for max records per group)](https://stackoverflow.com/questions/42636179/how-to-max-value-and-keep-all-columns-for-max-records-per-group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_max = df_all.groupBy(\"beat\").max(\"district\")\n",
    "df_new = df_max.join(df_all, [\"beat\"])\n",
    "df_new = df_new.filter(df_new['max(district)']==df_new['district']).select('beat','zip','district')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Delete Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop(df.col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple columns\n",
    "df = df.drop(\"address\", \"phoneNumber\")\n",
    "df = df.drop(df.address).drop(df.phoneNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **change column type to double**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/32284620/how-to-change-a-dataframe-column-from-string-type-to-double-type-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDf = df.withColumn(\"colName\", df[\"oldName\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newDf = df.withColumn(\"colName\", df[\"oldName\"].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Substring, get part of the string from original column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf #pyspark df column manipulation\n",
    "\n",
    "udf1 = udf(lambda x:x[0:2],StringType())\n",
    "df = df.withColumn('month',udf1('Date'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **random sample dataframe; subsample datarame by row**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pyspark sql doc](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample(withReplacement, fraction, seed=None)\n",
    "df.sample(False, 0.5, 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Transform pyspark column into numpy array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link](https://www.quora.com/How-do-I-convert-image-data-from-2D-array-to-1D-using-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q1_indegree = np.array(df4.select('in_degree').collect())\n",
    "\n",
    "# reshape 2D numpy array into 1D array\n",
    "q1_indegree = q1_indegree.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupby keep top 5, top n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/38397796/retrieve-top-n-in-each-group-of-a-dataframe-in-pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"user_1\",  \"object_1\",  3), \n",
    "                      (\"user_1\",  \"object_2\",  2), \n",
    "                      (\"user_2\",  \"object_1\",  5), \n",
    "                      (\"user_2\",  \"object_2\",  2), \n",
    "                      (\"user_2\",  \"object_2\",  6)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_id\", \"object_id\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|user_id|object_id|score|\n",
      "+-------+---------+-----+\n",
      "| user_1| object_1|    3|\n",
      "| user_1| object_2|    2|\n",
      "| user_2| object_1|    5|\n",
      "| user_2| object_2|    2|\n",
      "| user_2| object_2|    6|\n",
      "+-------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+----+\n",
      "|user_id|object_id|score|rank|\n",
      "+-------+---------+-----+----+\n",
      "| user_1| object_1|    3|   1|\n",
      "| user_1| object_2|    2|   2|\n",
      "| user_2| object_2|    6|   1|\n",
      "| user_2| object_1|    5|   2|\n",
      "+-------+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "\n",
    "window = Window.partitionBy(df['user_id']).orderBy(df['score'].desc())\n",
    "\n",
    "df.select('*', rank().over(window).alias('rank')) .filter(col('rank') <= 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **groupby join as list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|Name|         Age|\n",
      "+----+------------+\n",
      "|Mark|    [31, 32]|\n",
      "|John|[41, 42, 43]|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = sqlContext.createDataFrame([(31, \"Mark\"), (32, \"Mark\"), (41, \"John\"), (42, \"John\"), (43, \"John\")],[ 'Age', 'Name'])\n",
    "\n",
    "df_grouped = df.groupBy(\"Name\").agg(F.collect_list(F.col(\"Age\")).alias(\"Age\"))\n",
    "\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Union and intersect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"C\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  2)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_1\", \"user_2\", \"period\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     A|     B|\n",
      "|     A|     C|\n",
      "|     D|     C|\n",
      "|     B|     A|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     D|     C|\n",
      "|     A|     C|\n",
      "|     C|     A|\n",
      "|     C|     D|\n",
      "|     B|     A|\n",
      "|     A|     B|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').union(df.select('user_2','user_1')).drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|user_1|user_2|\n",
      "+------+------+\n",
      "|     B|     A|\n",
      "|     A|     B|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('user_1','user_2').intersect(df.select('user_2','user_1')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Calculate reciprocal of a network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"B\",  2),                       \n",
    "                      (\"A\",  \"C\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  2)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user1\", \"user2\", \"period\"])\n",
    "# answer \n",
    "# period 1: 0/3\n",
    "# period 2: 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+\n",
      "|user1|user2|period|\n",
      "+-----+-----+------+\n",
      "|    A|    B|     1|\n",
      "|    A|    B|     2|\n",
      "|    A|    C|     1|\n",
      "|    D|    C|     1|\n",
      "|    B|    A|     2|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reci_number = df.select('user1','user2').intersect(df.select('user2','user1')).count()/2\n",
    "reci_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reciprocal_ratio = reci_number/(df.drop_duplicates().count()-reci_number)\n",
    "reciprocal_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reci_list = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    filter_df = df.filter(df['period']<=i)\n",
    "    \n",
    "    reci_number = filter_df.select('user1','user2').intersect(filter_df.select('user2','user1')).count()/2\n",
    "    reciprocal_ratio = reci_number/(df.count()-reci_number)\n",
    "    \n",
    "    reci_list.append(reciprocal_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.3333333333333333]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reci_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Drop Duplicated**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(\"A\",  \"B\",  1), \n",
    "                      (\"A\",  \"B\",  1), \n",
    "                      (\"D\",  \"C\",  1), \n",
    "                      (\"B\",  \"A\",  1)])\n",
    "df = sqlContext.createDataFrame(rdd, [\"user_1\", \"user_2\", \"period\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|user_1|user_2|period|\n",
      "+------+------+------+\n",
      "|     A|     B|     1|\n",
      "|     A|     B|     1|\n",
      "|     D|     C|     1|\n",
      "|     B|     A|     1|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|user_1|user_2|period|\n",
      "+------+------+------+\n",
      "|     D|     C|     1|\n",
      "|     B|     A|     1|\n",
      "|     A|     B|     1|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **get len create new column pyspark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/44541605/how-to-get-the-lists-length-in-one-column-in-dataframe-spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|col1|col2|     col3|\n",
      "+----+----+---------+\n",
      "|   1|   A|[1, 2, 3]|\n",
      "|   2|   B|   [3, 5]|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "\n",
    "df = sqlContext.createDataFrame([(1, \"A\", [1,2,3]), (2, \"B\", [3,5])],[\"col1\", \"col2\", \"col3\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+--------+\n",
      "|col1|col2|     col3|col3_len|\n",
      "+----+----+---------+--------+\n",
      "|   1|   A|[1, 2, 3]|       3|\n",
      "|   2|   B|   [3, 5]|       2|\n",
      "+----+----+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('col3_len',size('col3')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+------------+\n",
      "|col1|col2|     col3|col3_is_zero|\n",
      "+----+----+---------+------------+\n",
      "|   1|   A|[1, 2, 3]|       false|\n",
      "|   2|   B|   [3, 5]|       false|\n",
      "+----+----+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('col3_is_zero',size('col3')==0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get string length as new column**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/36604951/add-new-column-string-length-to-df-by-userdefinedfunction-in-spark-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('your_column_length', F.length(your_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get column sum**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/47812526/pyspark-sum-a-column-in-dataframe-and-return-results-as-int?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will return a list\n",
    "df.groupBy().sum().collect()\n",
    "\n",
    "# return a single number\n",
    "df.groupBy().sum().collect()[0][0]\n",
    "\n",
    "# get the sum from the second column\n",
    "df.groupBy().sum().collect()[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/43968946/calculating-percentages-on-a-pyspark-dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Create Ratio for categorical feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = df6.groupby().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df7 = df6.withColumn('ratio', (df6[\"count\"] / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **create new column using the sum multiple columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/31955309/add-column-sum-as-new-column-in-pyspark-dataframe?rq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newdf = df.withColumn('total', sum(df[col] for col in df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Save list as txt in pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_as_txt(mylist, filename):\n",
    "    \"\"\"save list as txt file\"\"\" \n",
    "\n",
    "    thefile = open(filename, 'w')\n",
    "    for item in mylist:\n",
    "        thefile.write(\"%s\\n\" % str(item))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **get percentage of categories in a single column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   1|   A|\n",
      "|   2|   B|\n",
      "|   2|   B|\n",
      "|   2|   B|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame([(1, \"A\"), (2, \"B\"), (2, \"B\"), (2, \"B\")],[\"col1\", \"col2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby().sum().collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------------+\n",
      "|col1|count|             ratio|\n",
      "+----+-----+------------------+\n",
      "|   2|    3|               1.0|\n",
      "|   1|    1|0.3333333333333333|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get ratio for a numerical column\n",
    "df1 = df.groupby('col1').count()\n",
    "df1 = df1.sort(df1['count'].desc())\n",
    "df1 = df1.withColumn('ratio', (df1[\"count\"] / df1.groupby().sum().collect()[0][0]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get ratio for a categorical column\n",
    "df2 = df.groupby('col2').count()\n",
    "df2 = df2.sort(df2['count'].desc())\n",
    "df2 = df2.withColumn('ratio', (df2[\"count\"] / df2.groupby().sum().collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|col2|count|ratio|\n",
      "+----+-----+-----+\n",
      "|   B|    3| 0.75|\n",
      "|   A|    1| 0.25|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **transform dataframe into rdd**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myrdd = df.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='sql'>Spark SQL</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sparkSQL work with dataframe api, we need to firstly transform our file/RDD into spark dataframe\n",
    "* RDD -> dataframe\n",
    "* file -> dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **transform RDD into dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID,Case Number,Date,Block,IUCR,Primary Type,Description,Location Description,Arrest,Domestic,Beat,District,Ward,Community Area,FBI Code,X Coordinate,Y Coordinate,Year,Updated On,Latitude,Longitude,Location',\n",
       " '10078659,HY267429,05/19/2015 11:57:00 PM,010XX E 79TH ST,143A,WEAPONS VIOLATION,UNLAWFUL POSS OF HANDGUN,STREET,true,false,0624,006,8,44,15,1184626,1852799,2015,05/26/2015 12:42:06 PM,41.751242944,-87.599004724,\"(41.751242944, -87.599004724)\"',\n",
       " '10078598,HY267408,05/19/2015 11:50:00 PM,067XX N SHERIDAN RD,3731,INTERFERENCE WITH PUBLIC OFFICER,OBSTRUCTING IDENTIFICATION,STREET,true,false,2432,024,49,1,24,1167071,1944859,2015,05/26/2015 12:42:06 PM,42.004255918,-87.660691083,\"(42.004255918, -87.660691083)\"',\n",
       " '10078625,HY267417,05/19/2015 11:47:00 PM,026XX E 77TH ST,2170,NARCOTICS,POSSESSION OF DRUG EQUIPMENT,STREET,true,false,0421,004,7,43,18,1195299,1854463,2015,05/26/2015 12:42:06 PM,41.755552462,-87.559839339,\"(41.755552462, -87.559839339)\"',\n",
       " '10078662,HY267423,05/19/2015 11:46:00 PM,015XX E 62ND ST,051A,ASSAULT,AGGRAVATED: HANDGUN,APARTMENT,false,true,0314,003,5,42,04A,1187377,1864316,2015,05/26/2015 12:42:06 PM,41.782781732,-87.588558362,\"(41.782781732, -87.588558362)\"']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRDD3 = myRDD2.map(lambda x: x.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ID',\n",
       "  'Case Number',\n",
       "  'Date',\n",
       "  'Block',\n",
       "  'IUCR',\n",
       "  'Primary Type',\n",
       "  'Description',\n",
       "  'Location Description',\n",
       "  'Arrest',\n",
       "  'Domestic',\n",
       "  'Beat',\n",
       "  'District',\n",
       "  'Ward',\n",
       "  'Community Area',\n",
       "  'FBI Code',\n",
       "  'X Coordinate',\n",
       "  'Y Coordinate',\n",
       "  'Year',\n",
       "  'Updated On',\n",
       "  'Latitude',\n",
       "  'Longitude',\n",
       "  'Location'],\n",
       " ['10078659',\n",
       "  'HY267429',\n",
       "  '05/19/2015 11:57:00 PM',\n",
       "  '010XX E 79TH ST',\n",
       "  '143A',\n",
       "  'WEAPONS VIOLATION',\n",
       "  'UNLAWFUL POSS OF HANDGUN',\n",
       "  'STREET',\n",
       "  'true',\n",
       "  'false',\n",
       "  '0624',\n",
       "  '006',\n",
       "  '8',\n",
       "  '44',\n",
       "  '15',\n",
       "  '1184626',\n",
       "  '1852799',\n",
       "  '2015',\n",
       "  '05/26/2015 12:42:06 PM',\n",
       "  '41.751242944',\n",
       "  '-87.599004724',\n",
       "  '\"(41.751242944',\n",
       "  ' -87.599004724)\"']]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: string, _3: string, _4: string, _5: string, _6: string, _7: string, _8: string, _9: string, _10: string, _11: string, _12: string, _13: string, _14: string, _15: string, _16: string, _17: string, _18: string, _19: string, _20: string, _21: string, _22: string]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Case Number: string, Date: string, Block: string, IUCR: string, Primary Type: string, Description: string, Location Description: string, Arrest: string, Domestic: string, Beat: string, District: string, Ward: string, Community Area: string, FBI Code: string, X Coordinate: string, Y Coordinate: string, Year: string, Updated On: string, Latitude: string, Longitude: string, Location: string]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD3.toDF(['ID','Case Number','Date','Block','IUCR','Primary Type','Description','Location Description','Arrest','Domestic','Beat','District','Ward','Community Area','FBI Code','X Coordinate','Y Coordinate','Year','Updated On','Latitude','Longitude','Location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Write SQL code in SparkSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to write SQL code on SparkSQL\n",
    "df.registerTempTable(\"mydf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it will return a dataframe not a sql tempTable\n",
    "sqlContect.sql(\"select * from mydf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ml'>Machine Learning in PySpark</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Basic Statistics - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever we want to use Mllib, we need to convert our data into a specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(['0,1,2,3','1,4,3,4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,1,2,3', '1,4,3,4']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(\",\")]\n",
    "    return Vectors.dense(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = rdd.map(parsePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([0.0, 1.0, 2.0, 3.0]), DenseVector([1.0, 4.0, 3.0, 4.0])]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use pretty much anything provided in mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5  2.5  2.5  3.5]\n",
      "[ 0.5  4.5  0.5  0.5]\n",
      "[ 1.  2.  2.  2.]\n"
     ]
    }
   ],
   "source": [
    "summary = Statistics.colStats(rdd2)\n",
    "print(summary.mean())  # a dense vector containing the mean value for each column\n",
    "print(summary.variance())  # column-wise variance\n",
    "print(summary.numNonzeros())  # number of nonzeros in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <a id='lm'>**Linear Regression**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using mllib RDD based API**\n",
    "\n",
    "[Spark2.2: Linear Methods - RDD-based API](https://spark.apache.org/docs/2.2.0/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression)\n",
    "\n",
    "[Spark2.1: Evaluation Metrics - RDD-based API](https://spark.apache.org/docs/2.1.0/mllib-evaluation-metrics.html#regression-model-evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 3.401714059633831\n",
      "RMSE = 1.8443736225705005\n",
      "R-squared = -3.5295821141199974\n",
      "MAE = 1.6874236102185893\n",
      "Explained variance = 2.575165766694828\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.replace(',', ' ').split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/lpsa.data\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=10000, step=0.01)\n",
    "\n",
    "# # Evaluate the model on training data\n",
    "# valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "# MSE = valuesAndPreds \\\n",
    "#     .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "#     .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "# print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# mymean = valuesAndPreds.map(lambda x: x[0]).mean()\n",
    "# MST = valuesAndPreds.map(lambda vp: (vp[0] - mymean)**2).reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "# print(\"My R2 = \"+ str(1-(MSE/MST)))\n",
    "\n",
    "# Get predictions\n",
    "valuesAndPreds = parsedData.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics = RegressionMetrics(valuesAndPreds)\n",
    "\n",
    "# Squared Error\n",
    "print(\"MSE = %s\" % metrics.meanSquaredError)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)\n",
    "\n",
    "# Mean absolute error\n",
    "print(\"MAE = %s\" % metrics.meanAbsoluteError)\n",
    "\n",
    "# Explained variance\n",
    "print(\"Explained variance = %s\" % metrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(-0.4307829, [-1.63735562648,-2.00621178481,-1.86242597251,-1.02470580167,-0.522940888712,-0.863171185426,-1.04215728919,-0.864466507337]),\n",
       " LabeledPoint(-0.1625189, [-1.98898046127,-0.722008756122,-0.787896192088,-1.02470580167,-0.522940888712,-0.863171185426,-1.04215728919,-0.864466507337])]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mllib using DataFrame api**\n",
    "\n",
    "[Spark Doc](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#linear-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.322925166774,-0.343854803456,1.91560170235,0.0528805868039,0.76596272046,0.0,-0.151053926692,-0.215879303609,0.220253691888]\n",
      "Intercept: 0.1598936844239736\n",
      "numIterations: 7\n",
      "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -9.889232683103197|\n",
      "|  0.5533794340053554|\n",
      "|  -5.204019455758823|\n",
      "| -20.566686715507508|\n",
      "|    -9.4497405180564|\n",
      "|  -6.909112502719486|\n",
      "|  -10.00431602969873|\n",
      "|   2.062397807050484|\n",
      "|  3.1117508432954772|\n",
      "| -15.893608229419382|\n",
      "|  -5.036284254673026|\n",
      "|   6.483215876994333|\n",
      "|  12.429497299109002|\n",
      "|  -20.32003219007654|\n",
      "| -2.0049838218725005|\n",
      "| -17.867901734183793|\n",
      "|   7.646455887420495|\n",
      "| -2.2653482182417406|\n",
      "|-0.10308920436195645|\n",
      "|  -1.380034070385301|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 10.189077\n",
      "r2: 0.022861\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\").load(\"data/sample_linear_regression_data.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the response column should be with the name **label**, features should be all in **feature** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=-9.490009878824548, features=SparseVector(10, {0: 0.4551, 1: 0.3664, 2: -0.3826, 3: -0.4458, 4: 0.3311, 5: 0.8067, 6: -0.2624, 7: -0.4485, 8: -0.0727, 9: 0.5658})),\n",
       " Row(label=0.2577820163584905, features=SparseVector(10, {0: 0.8387, 1: -0.127, 2: 0.4998, 3: -0.2269, 4: -0.6452, 5: 0.1887, 6: -0.5805, 7: 0.6519, 8: -0.6556, 9: 0.1749}))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---+\n",
      "|label| x1| x2|\n",
      "+-----+---+---+\n",
      "|    1|  2|  3|\n",
      "|   -2|  1| 14|\n",
      "|    2|  9| 10|\n",
      "+-----+---+---+\n",
      "\n",
      "+-----+---+---+----------+\n",
      "|label| x1| x2|  features|\n",
      "+-----+---+---+----------+\n",
      "|    1|  2|  3| [2.0,3.0]|\n",
      "|   -2|  1| 14|[1.0,14.0]|\n",
      "|    2|  9| 10|[9.0,10.0]|\n",
      "+-----+---+---+----------+\n",
      "\n",
      "Coefficients: [0.299694346941,-0.176452130024]\n",
      "Intercept: 0.7226251157843492\n",
      "numIterations: 5\n",
      "objectiveHistory: [0.5000000000000001, 0.43797741912312027, 0.19304675519723025, 0.1930464869604053, 0.19304648527659074]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|0.20734258040505105|\n",
      "|-0.5519896423887509|\n",
      "| 0.3446470619837001|\n",
      "+-------------------+\n",
      "\n",
      "RMSE: 0.394320\n",
      "r2: 0.946177\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = sqlContext.createDataFrame([(1,2,3),(-2,1,14),(2,9,10)],['label','x1','x2'])\n",
    "df.show()\n",
    "\n",
    "\n",
    "##### Create train dataframe\n",
    "# to all the features in the column feature using vectorAssembler \n",
    "vectorAssembler = VectorAssembler(inputCols = ['x1','x2'], outputCol = 'features')\n",
    "train_df = vectorAssembler.transform(df)\n",
    "train_df.show()\n",
    "\n",
    "\n",
    "##### Fit model\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train_df)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **using pipeline to do data transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conceptual\n",
    "pip = [OneHot, Standardize, assembler, ols]\n",
    "pip.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Map Categorical feature into numeric value**\n",
    "\n",
    "StringIndexer encodes a string column of labels to a column of label indices\n",
    "\n",
    "[Spark Doc](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder-deprecated-since-230)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|category|\n",
      "+---+--------+\n",
      "|  0|       a|\n",
      "|  1|       b|\n",
      "|  2|       c|\n",
      "|  3|       a|\n",
      "|  4|       a|\n",
      "|  5|       c|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **One Hot Encoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Spark Doc](https://spark.apache.org/docs/latest/ml-features.html#onehotencoder-deprecated-since-230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+-------------+\n",
      "|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|\n",
      "|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|\n",
      "|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|\n",
      "|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|\n",
      "|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|\n",
      "+--------------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"categoryIndex1\", \"categoryIndex2\"])\n",
    "\n",
    "# the number of inputCols must be the same as #outputCols\n",
    "# input is the column that we want to do the one hot encoding\n",
    "# output is about the name of the one-hot-encoding feautre, which is in sparseVector foramt\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n",
    "                                 outputCols=[\"categoryVec1\", \"categoryVec2\"])\n",
    "model = encoder.fit(df)\n",
    "encoded = model.transform(df)\n",
    "encoded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <a id='kmeans'>**Kmeans Clustering**</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[mllib clustering](https://spark.apache.org/docs/1.6.0/mllib-clustering.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "\n",
    "# Load and parse the data as RDD\n",
    "data = sc.textFile(\"data/kmeans_data.txt\")\n",
    "parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  0.,  0.]),\n",
       " array([ 0.1,  0.1,  0.1]),\n",
       " array([ 0.2,  0.2,  0.2]),\n",
       " array([ 9.,  9.,  9.]),\n",
       " array([ 9.1,  9.1,  9.1]),\n",
       " array([ 9.2,  9.2,  9.2])]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/mllib/clustering.py:347: UserWarning: The param `runs` has no effect since Spark 2.0.0.\n",
      "  warnings.warn(\"The param `runs` has no effect since Spark 2.0.0.\")\n"
     ]
    }
   ],
   "source": [
    "# Build the model (cluster the data)\n",
    "clusters = KMeans.train(parsedData, 2, maxIterations=10,\n",
    "        runs=10, initializationMode=\"random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.1,  0.1,  0.1]), array([ 9.1,  9.1,  9.1])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Error = 0.6928203230275498\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "def error(point):\n",
    "    center = clusters.centers[clusters.predict(point)]\n",
    "    return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "# # Save and load model\n",
    "# clusters.save(sc, \"myModelPath\")\n",
    "# sameModel = KMeansModel.load(sc, \"myModelPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cluster_number(cluster_list, myrdd, maxIterations, runs):\n",
    "    \"\"\"try out different cluster number for kmeans and get the SSE\"\"\"\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    def error(point):\n",
    "        center = clusters.centers[clusters.predict(point)]\n",
    "        return sqrt(sum([x**2 for x in (point - center)]))\n",
    "    \n",
    "    sse_list = []\n",
    "    for i in cluster_list:\n",
    "        # Build the model (cluster the data)\n",
    "        clusters = KMeans.train(myrdd, i, maxIterations=maxIterations,\n",
    "                runs=runs, initializationMode=\"random\",seed=123)        \n",
    "\n",
    "        WSSSE = myrdd.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "        sse_list.append(WSSSE)\n",
    "        print(\"For cluster = \"+str(i)+\", the within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "    return sse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cluster = 2, the within Set Sum of Squared Error = 0.6928203230275498\n",
      "For cluster = 3, the within Set Sum of Squared Error = 0.5196152422706619\n",
      "For cluster = 4, the within Set Sum of Squared Error = 0.34641016151377485\n",
      "For cluster = 5, the within Set Sum of Squared Error = 0.17320508075688773\n",
      "For cluster = 6, the within Set Sum of Squared Error = 0.0\n",
      "For cluster = 7, the within Set Sum of Squared Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "cluster_list = range(2,8)\n",
    "sse_list = find_cluster_number(cluster_list=cluster_list, \n",
    "                         myrdd=parsedData, \n",
    "                         maxIterations=10, \n",
    "                         runs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e6c6278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in cluster_list], sse_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using pyspark.ml package**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spark.ml doc](https://spark.apache.org/docs/2.2.0/ml-clustering.html#k-means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error(feature, prediction, centers):\n",
    "    center = centers[prediction]\n",
    "    return (prediction, sqrt(np.sum(((feature - center).values)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+\n",
      "|col1|col2|col3|     features|\n",
      "+----+----+----+-------------+\n",
      "| 0.0| 0.0| 0.1|[0.0,0.0,0.1]|\n",
      "| 0.1| 0.1| 0.1|[0.1,0.1,0.1]|\n",
      "| 0.2| 0.2| 0.2|[0.2,0.2,0.2]|\n",
      "| 9.0| 9.0| 9.0|[9.0,9.0,9.0]|\n",
      "| 9.1| 9.1| 9.1|[9.1,9.1,9.1]|\n",
      "| 9.2| 9.2| 9.2|[9.2,9.2,9.2]|\n",
      "+----+----+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### the input is a dataframe\n",
    "df = sqlContext.createDataFrame([\n",
    "    (0.0, 0.0, 0.1),\n",
    "    (0.1, 0.1, 0.1),\n",
    "    (0.2, 0.2, 0.2),\n",
    "    (9.0, 9.0, 9.0),\n",
    "    (9.1, 9.1, 9.1),\n",
    "    (9.2, 9.2, 9.2)\n",
    "], [\"col1\", \"col2\", \"col3\"])\n",
    "\n",
    "##### Create train dataframe\n",
    "# using features in the column feature using vectorAssembler \n",
    "vectorAssembler = VectorAssembler(inputCols = ['col1','col2','col3'], outputCol = 'features')\n",
    "train_df = vectorAssembler.transform(df)\n",
    "train_df2 = train_df.select('features')\n",
    "\n",
    "train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|     features|\n",
      "+-------------+\n",
      "|[0.0,0.0,0.1]|\n",
      "|[0.1,0.1,0.1]|\n",
      "|[0.2,0.2,0.2]|\n",
      "|[9.0,9.0,9.0]|\n",
      "|[9.1,9.1,9.1]|\n",
      "|[9.2,9.2,9.2]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([0.0, 0.0, 0.1])),\n",
       " Row(features=DenseVector([0.1, 0.1, 0.1]))]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col1: double, col2: double, col3: double, features: vector]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 0.10666666666661213\n",
      "Cluster Centers: \n",
      "[ 9.1  9.1  9.1]\n",
      "[ 0.1         0.1         0.13333333]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "# when using compute cost, we can use dataframe with all the features.\n",
    "wssse = model.computeCost(train_df)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6813873182925777"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an alternative to calculate SSE if computeCost is not available\n",
    "# when using self-defined function, the input can only be the dataframe with feature column\n",
    "def error(feature, prediction, centers):\n",
    "        center = centers[prediction]\n",
    "        print(feature)\n",
    "        print(center)\n",
    "        return (prediction, sqrt(np.sum(((feature - center).values)**2))) \n",
    "\n",
    "transformed = model.transform(train_df2).rdd.map(tuple)\n",
    "wssse = transformed.map(lambda point: error(point[0],point[1],centers)).values().sum()\n",
    "wssse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cluster_number(cluster_list, mydf, seed):\n",
    "    \"\"\"try out different cluster number for kmeans and get the SSE\"\"\"\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    def error(feature, prediction, centers):\n",
    "        center = centers[prediction]\n",
    "        return (prediction, sqrt(np.sum(((feature - center).values)**2)))\n",
    "    \n",
    "    sse_list = []\n",
    "    for i in cluster_list:\n",
    "        # Build the model (cluster the data)\n",
    "        kmeans = KMeans().setK(i).setSeed(seed)\n",
    "        model = kmeans.fit(mydf)\n",
    "        \n",
    "        # cluster center\n",
    "        centers = model.clusterCenters()\n",
    "        \n",
    "        transformed = model.transform(mydf).rdd.map(tuple)\n",
    "        wssse = transformed.map(lambda point: error(point[0],point[1],centers)).values().sum()\n",
    "        sse_list.append(wssse)\n",
    "        print(\"For cluster = \"+str(i)+\", the within Set Sum of Squared Error = \" + str(wssse))                \n",
    "       \n",
    "    return sse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+----------+\n",
      "|col1|col2|col3|     features|prediction|\n",
      "+----+----+----+-------------+----------+\n",
      "| 0.0| 0.0| 0.1|[0.0,0.0,0.1]|         1|\n",
      "| 0.1| 0.1| 0.1|[0.1,0.1,0.1]|         1|\n",
      "| 0.2| 0.2| 0.2|[0.2,0.2,0.2]|         1|\n",
      "| 9.0| 9.0| 9.0|[9.0,9.0,9.0]|         0|\n",
      "| 9.1| 9.1| 9.1|[9.1,9.1,9.1]|         0|\n",
      "| 9.2| 9.2| 9.2|[9.2,9.2,9.2]|         0|\n",
      "+----+----+----+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(train_df).show()#.rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For cluster = 2, the within Set Sum of Squared Error = 0.6813873182925777\n",
      "For cluster = 3, the within Set Sum of Squared Error = 0.5081822375356906\n",
      "For cluster = 4, the within Set Sum of Squared Error = 0.3146264369941966\n",
      "For cluster = 5, the within Set Sum of Squared Error = 0.14142135623730953\n",
      "For cluster = 6, the within Set Sum of Squared Error = 0.0\n",
      "For cluster = 7, the within Set Sum of Squared Error = 0.0\n"
     ]
    }
   ],
   "source": [
    "cluster_list = range(2,8)\n",
    "sse_list = find_cluster_number(cluster_list, train_df2, 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e7944e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in cluster_list], sse_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_kmeans(cluster_num, mydf, seed):\n",
    "    \n",
    "    # Trains a k-means model.\n",
    "    kmeans = KMeans().setK(2).setSeed(seed)\n",
    "    model = kmeans.fit(mydf)\n",
    "    \n",
    "    # add cluster result\n",
    "    final_df = model.transform(mydf)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-------------+----------+\n",
      "|col1|col2|col3|     features|prediction|\n",
      "+----+----+----+-------------+----------+\n",
      "| 0.0| 0.0| 0.1|[0.0,0.0,0.1]|         0|\n",
      "| 0.1| 0.1| 0.1|[0.1,0.1,0.1]|         0|\n",
      "| 0.2| 0.2| 0.2|[0.2,0.2,0.2]|         0|\n",
      "| 9.0| 9.0| 9.0|[9.0,9.0,9.0]|         1|\n",
      "| 9.1| 9.1| 9.1|[9.1,9.1,9.1]|         1|\n",
      "| 9.2| 9.2| 9.2|[9.2,9.2,9.2]|         1|\n",
      "+----+----+----+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = fit_kmeans(2, train_df, 123)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Normalize column for pyspark dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame([\n",
    "    (0.0, 1.0),\n",
    "    (1.0, 0.0),\n",
    "    (2.0, 1.0),\n",
    "    (0.0, 2.0),\n",
    "    (0.0, 1.0),\n",
    "    (2.0, 0.0)\n",
    "], [\"col1\", \"col2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 0.0| 1.0|\n",
      "| 1.0| 0.0|\n",
      "| 2.0| 1.0|\n",
      "| 0.0| 2.0|\n",
      "| 0.0| 1.0|\n",
      "| 2.0| 0.0|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/47624129/how-to-standardize-one-column-in-spark-using-standardscaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, mean, col\n",
    "\n",
    "sample_df = sqlContext.createDataFrame([(1, ), (2, ), (3, )]).toDF(\"age\")\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nor_df = sample_df.select(mean(\"age\").alias(\"mean_age\"), stddev(\"age\").alias(\"stddev_age\")).crossJoin(sample17).withColumn(\"age_scaled\" , (col(\"age\") - col(\"mean_age\")) / col(\"stddev_age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nor_df2 = nor_df.select('age','age_scaled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|age|age_scaled|\n",
      "+---+----------+\n",
      "|  1|      -1.0|\n",
      "|  2|       0.0|\n",
      "|  3|       1.0|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nor_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "|age|age_scaled|\n",
      "+---+----------+\n",
      "|  1|      -1.0|\n",
      "|  2|       0.0|\n",
      "|  3|       1.0|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean_age, sttdev_age = sample_df.select(mean(\"age\"), stddev(\"age\")).first()\n",
    "\n",
    "sample_df.withColumn(\"age_scaled\", (col(\"age\") - mean_age) / sttdev_age).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[StackOverFlow](https://stackoverflow.com/questions/40337744/scalenormalise-a-column-in-spark-dataframe-pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spark.ml doc: normalization](https://spark.apache.org/docs/1.6.0/ml-features.html#normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probably need to vectorize the columns\n",
    "\n",
    "# from pyspark.ml.feature import MinMaxScaler\n",
    "# scaler = MinMaxScaler(inputCol=\"col1\", outputCol=\"col1_nor\", p=1.0)\n",
    "# scalerModel = scaler.fit(df)\n",
    "# scaledData = scalerModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probably need to vectorize the columns\n",
    "\n",
    "# from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# dataFrame = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "# # Normalize each Vector using $L^1$ norm.\n",
    "# normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\n",
    "# l1NormData = normalizer.transform(dataFrame)\n",
    "# l1NormData.show()\n",
    "\n",
    "# # Normalize each Vector using $L^\\infty$ norm.\n",
    "# lInfNormData = normalizer.transform(dataFrame, {normalizer.p: float(\"inf\")})\n",
    "# lInfNormData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <a id='als'>**RecSys: Alternative Least Square**</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and parse the data\n",
    "train = sc.textFile(\"data/alstrain.data\")\n",
    "trainratings = train.map(lambda l: l.split(',')).map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sc.textFile(\"data/alstest.data\")\n",
    "testratings = test.map(lambda l: l.split(',')).map(lambda l: (int(l[0]), int(l[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=5.0),\n",
       " Rating(user=1, product=2, rating=1.0),\n",
       " Rating(user=1, product=3, rating=5.0),\n",
       " Rating(user=1, product=4, rating=1.0),\n",
       " Rating(user=2, product=1, rating=5.0),\n",
       " Rating(user=2, product=2, rating=1.0),\n",
       " Rating(user=2, product=3, rating=5.0),\n",
       " Rating(user=2, product=4, rating=1.0),\n",
       " Rating(user=3, product=1, rating=1.0),\n",
       " Rating(user=3, product=2, rating=5.0),\n",
       " Rating(user=3, product=3, rating=1.0),\n",
       " Rating(user=3, product=4, rating=5.0),\n",
       " Rating(user=4, product=1, rating=1.0),\n",
       " Rating(user=4, product=2, rating=5.0),\n",
       " Rating(user=4, product=3, rating=1.0)]"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 4)]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testratings.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using Alternating Least Squares\n",
    "rank = 10\n",
    "numIterations = 10\n",
    "model = ALS.train(trainratings, rank, numIterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on training data\n",
    "# testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testratings).map(lambda r: ((r[0], r[1]), r[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((4, 4), 3.9456003394711496)]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 1.5634715432297673e-05\n"
     ]
    }
   ],
   "source": [
    "ratesAndPreds = testratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "# model.save(sc, \"target/tmp/myCollaborativeFilter\")\n",
    "# sameModel = MatrixFactorizationModel.load(sc, \"target/tmp/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 3), (1.0, 1.0007994972612164)),\n",
       " ((4, 2), (5.0, 4.994243054218451)),\n",
       " ((1, 2), (1.0, 1.0000286098526114)),\n",
       " ((1, 3), (5.0, 4.997274399460885)),\n",
       " ((1, 4), (1.0, 1.0001120609765135))]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratesAndPreds.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='refer'>Reference</a>\n",
    "\n",
    "* [Complete Guide on DataFrame Operations in PySpark](https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/)\n",
    "* [Introduction to DataFrames - Python](https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-python.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
